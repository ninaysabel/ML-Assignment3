{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f7ef20f0-722f-4240-8a79-437d4a3b8832",
      "metadata": {
        "id": "f7ef20f0-722f-4240-8a79-437d4a3b8832"
      },
      "source": [
        "## Assignment 3: $k$ Nearest Neighbor and $k$ Means Clustering\n",
        "\n",
        "## **Do three questions.**\n",
        "\n",
        "`! git clone https://www.github.com/DS3001/assignment3`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://www.github.com/DS3001/assignment3"
      ],
      "metadata": {
        "id": "mZXo2Rx6quaq",
        "outputId": "1a1d2f26-d1df-460d-ca46-8e7f78a9c0f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mZXo2Rx6quaq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'assignment3'...\n",
            "warning: redirecting to https://github.com/DS3001/assignment3.git/\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 14 (delta 2), reused 1 (delta 1), pack-reused 7\u001b[K\n",
            "Receiving objects: 100% (14/14), 868.21 KiB | 5.95 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0f34497-2d71-4705-ac63-c0532e545022",
      "metadata": {
        "id": "d0f34497-2d71-4705-ac63-c0532e545022"
      },
      "source": [
        "**Q1.** This question is a case study for $k$ nearest neighbor. The target variable `y` is `Purchase` and the features of interest are `Age` and `AnnualSalary`.\n",
        "\n",
        "1. Load the `./data/car_data.csv` data. Look at the head and dimensions of the data.\n",
        "2. Summarize the variables (`User.ID`, `Gender`, `Age`, `AnnualSalary`, `Purchase`). Are there any missings to clean? Convert the `Gender` categorical variable into a dummy variable that takes the value 0 for male and 1 for female. Create a matrix $X$ of predictors including `Age` and `AnnualSalary`, and an outcome $y$ equaling `Purchase`.\n",
        "3. MaxMin-normalize `Age` and `AnnualSalary` in `X`.\n",
        "4. Split the sample into a ~80% training dataset and a ~20% testing dataset.\n",
        "5. Treat this as a classification problem: The model is supposed to predict 0 or 1 for each customer, classifying them as a purchaser or non-purchaser. Use sklearn to determine the optimal number of neighbors $k$ to use.\n",
        "6. Run the model for the optimal number of neighbors on the testing data. Cross tabulate the predicted outcomes against the actual outcomes; this is called a **confusion matrix**. How often does the model predict a sale when one fails to occur? How often does the model predict no sale when one does occur? Overall, does it provide accurate predictions?\n",
        "7. Now, compute confusion matrices separately for men and women, as in part 6. Does the model make more accurate predictions for one sex or the other? Explain. (Performance of algorithms on population subgroups is a growing topic in data science.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "car = pd.read_csv('./assignment3/data/car_data.csv')\n",
        "print(car.columns)\n",
        "car.head()\n",
        "\n",
        "#2 Matrix\n",
        "car.describe()\n",
        "car['gdummy'] = 0\n",
        "car.loc[ car['Gender']== 'Female', 'gdummy'] = 1\n",
        "car['gdummy'].value_counts()\n",
        "\n",
        "matrix = ['AnnualSalary', 'Age']\n",
        "X = car.loc[:,matrix]\n",
        "Y = car['Purchased']\n",
        "\n",
        "#3 MaxMin-normalize\n",
        "def maxmin(x):\n",
        "  x = (x-min(x))/(max(x)-min(x))\n",
        "  return x\n",
        "X = X.apply(maxmin)\n",
        "X.describe()\n",
        "\n",
        "#4 Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=.2,random_state=1)\n",
        "\n",
        "#5 Optimal k (reference from provided solution bc i got really stumped on this one)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "k_bar = 30\n",
        "k_grid = np.arange(1,k_bar)\n",
        "SSE = np.zeros(k_bar)\n",
        "for k in range(k_bar):\n",
        "  model = KNeighborsClassifier(n_neighbors=k+1)\n",
        "  fitted_model = model.fit(X_train,y_train)\n",
        "  y_hat = fitted_model.predict(X_test)\n",
        "  SSE[k] = np.sum( (y_test-y_hat)**2 )\n",
        "SSE_min = np.min(SSE)\n",
        "min_index = np.where(SSE==SSE_min)\n",
        "k_star = k_grid[min_index]\n",
        "print(k_star)\n",
        "\n",
        "plt.plot(np.arange(0,k_bar),SSE,label='Test')\n",
        "plt.xlabel(\"k\")\n",
        "plt.ylabel(\"SSE\")\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('SSE- k neighbors')\n",
        "plt.show()\n",
        "\n",
        "#6 Confusion Matrix\n",
        "confusion = KNeighborsClassifier(n_neighbors=9)\n",
        "conmat = confusion.fit(X_train,y_train)\n",
        "y_hat = conmat.predict(X_test)\n",
        "pd.crosstab(y_test, y_hat)\n",
        "# when a sale is not made, the model accurately predicts it 114 times, but when a sale is made but the model predicts that one wasn't, the model predicts this inaccurately 22 times\n",
        "# when a sale is made, the model accurately predicts it 53 times, when a sale is not made but the model predicts that one was, it happens 11 times\n",
        "\n",
        "#7 Confusion Matrix by Sex\n",
        "from sklearn.model_selection import train_test_split\n",
        "bygender = ['Gender','Age','AnnualSalary']\n",
        "X = car.loc[:,bygender]\n",
        "y = car['Purchased']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2,random_state=1)\n",
        "\n",
        "genmat = KNeighborsClassifier(n_neighbors=9)\n",
        "genmat_con = genmat.fit(X_train.drop('Gender',axis=1),y_train)\n",
        "y_hat = genmat_con.predict(X_test.drop('Gender',axis=1))\n",
        "\n",
        "My_hat = y_hat[ X_test['Gender'] == 'Male']\n",
        "Fy_hat = y_hat[ X_test['Gender'] == 'Female']\n",
        "My = y_test[ X_test['Gender'] == 'Male']\n",
        "Fy = y_test[ X_test['Gender'] == 'Female']\n",
        "\n",
        "pd.crosstab(Fy, Fy_hat)\n",
        "# the model predicts accurately for women about .76 times (80/105)\n",
        "pd.crosstab(My, My_hat)\n",
        "# the model predicts accurately for men about .83 times (81/97) showing that there is a higher accuracy rate for men over women just comparing the rates alone\n",
        "# calculating the difference between the accuracy rates by gender (.83-.76)/.6796 = .1084) there is about an 11% difference between gender, which is kind of a high differential to be had on the basis of gender alone"
      ],
      "metadata": {
        "id": "QJNBOCjvz_wx"
      },
      "id": "QJNBOCjvz_wx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "71c9e0b8-17f5-4ff9-9c76-2034bffe8d5c",
      "metadata": {
        "id": "71c9e0b8-17f5-4ff9-9c76-2034bffe8d5c"
      },
      "source": [
        "**Q2.** This question is a case study for $k$ nearest neighbor The target variable `y` is `price` and the features are `year` and `mileage`.\n",
        "\n",
        "1. Load the `./data/USA_cars_datasets.csv`. Keep the following variables and drop the rest: `price`, `year`, `mileage`. Are there any `NA`'s to handle? Look at the head and dimensions of the data.\n",
        "2. Maxmin normalize `year` and `mileage`.\n",
        "3. Split the sample into ~80% for training and ~20% for evaluation.\n",
        "4. Use the $k$NN algorithm for regression and the training data to predict `price` using `year` and `mileage` for the test set for $k=3,10,25,50,100,300$. For each value of $k$, compute the Sum of Squared Error and make a scatterplot showing the test value plotted against the predicted value. What patterns do you notice as you increase $k$?\n",
        "5. Determine the optimal $k$ for these data.\n",
        "6. Describe what happened in the plots of predicted versus actual prices as $k$ varied, taking your answer into part 6 into account. (Hint: Use the words \"underfitting\" and \"overfitting\".)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsRegressor as kNNRegression\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "5U722uegFl0_"
      },
      "id": "5U722uegFl0_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Question 2 Part 1*"
      ],
      "metadata": {
        "id": "HmVbydMOFvoz"
      },
      "id": "HmVbydMOFvoz"
    },
    {
      "cell_type": "code",
      "source": [
        "usa = pd.read_csv('/content/assignment3/data/USA_cars_datasets.csv')\n",
        "usa_cars = usa.drop(['brand', 'model', 'title_status', 'color', 'vin', 'lot', 'state', 'country', 'condition'], axis=1)\n",
        "usa_cars.isnull().values.any() #no NA values bc it runs false\n",
        "usa_cars.head()"
      ],
      "metadata": {
        "id": "mKvfyVm6F0-H"
      },
      "id": "mKvfyVm6F0-H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Question 2 Part 2 (Maxmin)*"
      ],
      "metadata": {
        "id": "zpdko4EHMew4"
      },
      "id": "zpdko4EHMew4"
    },
    {
      "cell_type": "code",
      "source": [
        "y = usa_cars['price']\n",
        "x = usa_cars.loc[:,['year','mileage']]\n",
        "\n",
        "def maxmin(x):\n",
        "    x = (x-min(x))/(max(x)-min(x))\n",
        "    return(x)\n",
        "\n",
        "X = x.apply(maxmin)\n",
        "X.describe()"
      ],
      "metadata": {
        "id": "HuYRSCGBMhIU"
      },
      "id": "HuYRSCGBMhIU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Question 2 Part 3 (split)*"
      ],
      "metadata": {
        "id": "gWQtJkrIO5IS"
      },
      "id": "gWQtJkrIO5IS"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2,random_state=1)"
      ],
      "metadata": {
        "id": "2qDWvX1ZO-QY"
      },
      "id": "2qDWvX1ZO-QY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Question 2 Part 4 (regression)*"
      ],
      "metadata": {
        "id": "h2PVRbzXPMGM"
      },
      "id": "h2PVRbzXPMGM"
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Value 3\n",
        "# as the K value increases, sse decreases\n",
        "\n",
        "k = 3 #set k-value\n",
        "model = kNNRegression(n_neighbors=k)\n",
        "fitted_model = model.fit(X_train, y_train)\n",
        "\n",
        "y_hat = fitted_model.predict(X_test) #predicting price\n",
        "sse = mean_squared_error(y_test, y_hat) * len(y_test) #calculating sse\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_hat)\n",
        "plt.title(f'Scatterplot for k={k}')\n",
        "plt.xlabel('Actual Price')\n",
        "plt.ylabel('Predicted Price')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n",
        "print(f'k={k}, SSE={sse}')\n"
      ],
      "metadata": {
        "id": "x9PYMKjRbL50"
      },
      "id": "x9PYMKjRbL50",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Value 10\n",
        "\n",
        "k = 10\n",
        "model = kNNRegression(n_neighbors=k)\n",
        "fitted_model = model.fit(X_train, y_train)\n",
        "\n",
        "y_hat = fitted_model.predict(X_test)\n",
        "sse = mean_squared_error(y_test, y_hat) * len(y_test)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_hat)\n",
        "plt.title(f'Scatterplot for k={k}')\n",
        "plt.xlabel('Actual Price')\n",
        "plt.ylabel('Predicted Price')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n",
        "print(f'k={k}, SSE={sse}')"
      ],
      "metadata": {
        "id": "kqVyFyO_ba62"
      },
      "id": "kqVyFyO_ba62",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Value 25\n",
        "k = 25\n",
        "model = kNNRegression(n_neighbors=k)\n",
        "fitted_model = model.fit(X_train, y_train)\n",
        "\n",
        "y_hat = fitted_model.predict(X_test)\n",
        "sse = mean_squared_error(y_test, y_hat) * len(y_test)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_hat)\n",
        "plt.title(f'Scatterplot for k={k}')\n",
        "plt.xlabel('Actual Price')\n",
        "plt.ylabel('Predicted Price')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n",
        "print(f'k={k}, SSE={sse}')"
      ],
      "metadata": {
        "id": "_p5D-fHLdEBD"
      },
      "id": "_p5D-fHLdEBD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Value 50\n",
        "k = 50\n",
        "model = kNNRegression(n_neighbors=k)\n",
        "fitted_model = model.fit(X_train, y_train)\n",
        "\n",
        "y_hat = fitted_model.predict(X_test)\n",
        "sse = mean_squared_error(y_test, y_hat) * len(y_test)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_hat)\n",
        "plt.title(f'Scatterplot for k={k}')\n",
        "plt.xlabel('Actual Price')\n",
        "plt.ylabel('Predicted Price')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n",
        "print(f'k={k}, SSE={sse}')"
      ],
      "metadata": {
        "id": "P9oRPWYkdXaj"
      },
      "id": "P9oRPWYkdXaj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Value 100\n",
        "k = 100\n",
        "model = kNNRegression(n_neighbors=k)\n",
        "fitted_model = model.fit(X_train, y_train)\n",
        "\n",
        "y_hat = fitted_model.predict(X_test)\n",
        "sse = mean_squared_error(y_test, y_hat) * len(y_test)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_hat)\n",
        "plt.title(f'Scatterplot for k={k}')\n",
        "plt.xlabel('Actual Price')\n",
        "plt.ylabel('Predicted Price')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n",
        "print(f'k={k}, SSE={sse}')"
      ],
      "metadata": {
        "id": "DMCZ9qThdbgw"
      },
      "id": "DMCZ9qThdbgw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Value 300\n",
        "k = 300\n",
        "model = kNNRegression(n_neighbors=k)\n",
        "fitted_model = model.fit(X_train, y_train)\n",
        "\n",
        "y_hat = fitted_model.predict(X_test)\n",
        "sse = mean_squared_error(y_test, y_hat) * len(y_test)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_hat)\n",
        "plt.title(f'Scatterplot for k={k}')\n",
        "plt.xlabel('Actual Price')\n",
        "plt.ylabel('Predicted Price')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n",
        "print(f'k={k}, SSE={sse}')"
      ],
      "metadata": {
        "id": "pvzM5Uf4dghr"
      },
      "id": "pvzM5Uf4dghr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Question 2 Part 5*"
      ],
      "metadata": {
        "id": "MQ-xSiCzgrpt"
      },
      "id": "MQ-xSiCzgrpt"
    },
    {
      "cell_type": "code",
      "source": [
        "k_bar = 70\n",
        "k_grid = np.arange(1,k_bar)\n",
        "k_star = k_grid[min_index]\n",
        "print('Optimal K-value:',k_star)\n",
        "\n",
        "#optimal k-value is 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wW3wbnbuguRn",
        "outputId": "87a64210-6007-4e56-dfea-19e6388463cc"
      },
      "id": "wW3wbnbuguRn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal K-value: [3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Question 2 Part 6*\n",
        "- When k=300, the graph had the potential to underfit the data and portraying an accurate SSE. Because 300 is such a large value, it has the tendency to oversimplify data, assuming a greater amount of data per value and thus, failing to account for any \"nuances\"\n",
        "- when k=3, the graph was able to show a lot of variability as the amount of \"neighbors\" considered was not a large group."
      ],
      "metadata": {
        "id": "7LN6SJPfhJTA"
      },
      "id": "7LN6SJPfhJTA"
    },
    {
      "cell_type": "markdown",
      "id": "d765b942-0ddf-4d42-adbf-8b64eeabf4c9",
      "metadata": {
        "id": "d765b942-0ddf-4d42-adbf-8b64eeabf4c9"
      },
      "source": [
        "**Q6.** This is a question about $k$ means clustering. We want to investigate how adjusting the \"noisiness\" of the data impacts the quality of the algorithm and the difficulty of picking $k$.\n",
        "\n",
        "1. Run the code below, which creates four datasets: `df0_125`, `df0_25`, `df0_5`, `df1_0`, and `df2_0`. Each data set is created by increasing the amount of `noise` (standard deviation) around the cluster centers, from `0.125` to `0.25` to `0.5` to `1.0` to `2.0`.\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def createData(noise,N=50):\n",
        "    np.random.seed(100) # Set the seed for replicability\n",
        "    # Generate (x1,x2,g) triples:\n",
        "    X1 = np.array([np.random.normal(1,noise,N),np.random.normal(1,noise,N)])\n",
        "    X2 = np.array([np.random.normal(3,noise,N),np.random.normal(2,noise,N)])\n",
        "    X3 = np.array([np.random.normal(5,noise,N),np.random.normal(3,noise,N)])\n",
        "    # Concatenate into one data frame\n",
        "    gdf1 = pd.DataFrame({'x1':X1[0,:],'x2':X1[1,:],'group':'a'})\n",
        "    gdf2 = pd.DataFrame({'x1':X2[0,:],'x2':X2[1,:],'group':'b'})\n",
        "    gdf3 = pd.DataFrame({'x1':X3[0,:],'x2':X3[1,:],'group':'c'})\n",
        "    df = pd.concat([gdf1,gdf2,gdf3],axis=0)\n",
        "    return df\n",
        "\n",
        "df0_125 = createData(0.125)\n",
        "df0_25 = createData(0.25)\n",
        "df0_5 = createData(0.5)\n",
        "df1_0 = createData(1.0)\n",
        "df2_0 = createData(2.0)\n",
        "```\n",
        "\n",
        "2. Make scatterplots of the $(X1,X2)$ points by group for each of the datasets. As the `noise` goes up from 0.125 to 2.0, what happens to the visual distinctness of the clusters?\n",
        "3. Create a scree plot for each of the datasets. Describe how the level of `noise` affects the scree plot (particularly the presence of a clear \"elbow\") and your ability to definitively select a $k$.\n",
        "4. Explain the intuition of the elbow, using this numerical simulation as an example."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Question 6 Part 1*"
      ],
      "metadata": {
        "id": "-2WPdFhliYMe"
      },
      "id": "-2WPdFhliYMe"
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6 Part 1\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def createData(noise,N=50):\n",
        "    np.random.seed(100)\n",
        "\n",
        "    trip_X1 = np.array([np.random.normal(1,noise,N),np.random.normal(1,noise,N)])\n",
        "    trip_X2 = np.array([np.random.normal(3,noise,N),np.random.normal(2,noise,N)])\n",
        "    trip_X3 = np.array([np.random.normal(5,noise,N),np.random.normal(3,noise,N)])\n",
        "\n",
        "    con_1 = pd.DataFrame({'x1':trip_X1[0,:],'x2':trip_X1[1,:],'group':'a'})\n",
        "    con_2 = pd.DataFrame({'x1':trip_X2[0,:],'x2':trip_X2[1,:],'group':'b'})\n",
        "    con_3 = pd.DataFrame({'x1':trip_X3[0,:],'x2':trip_X3[1,:],'group':'c'})\n",
        "    con_whole = pd.concat([con_1,con_2,con_3],axis=0)\n",
        "    return con_whole\n",
        "\n",
        "cw0_125 = createData(0.125)\n",
        "cw0_25 = createData(0.25)\n",
        "cw0_5 = createData(0.5)\n",
        "cw1_0 = createData(1.0)\n",
        "cw2_0 = createData(2.0)"
      ],
      "metadata": {
        "id": "q7pcTEhLXpW-"
      },
      "id": "q7pcTEhLXpW-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Question 6 Part 2*"
      ],
      "metadata": {
        "id": "X2HhDdFxiPcp"
      },
      "id": "X2HhDdFxiPcp"
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6 Part 2\n",
        "# As the noise increases by increments, the visual distinctness of the clusters begin to get more dispersed and overlap.\n",
        "# 0.125 Scatterplot\n",
        "sns.scatterplot(data = cw0_125, x = 'x1',y='x2',hue='group',style='group')"
      ],
      "metadata": {
        "id": "zmFpQEFDb5CW"
      },
      "id": "zmFpQEFDb5CW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.25 Scatterplot\n",
        "sns.scatterplot(data = cw0_25, x = 'x1',y='x2',hue='group',style='group')"
      ],
      "metadata": {
        "id": "hbWviYQBcfmc"
      },
      "id": "hbWviYQBcfmc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.5 Scatterplot\n",
        "sns.scatterplot(data = cw0_5, x = 'x1',y='x2',hue='group',style='group')"
      ],
      "metadata": {
        "id": "CTldtNYUco6a"
      },
      "id": "CTldtNYUco6a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.0 Scatterplot\n",
        "sns.scatterplot(data = cw1_0, x = 'x1',y='x2',hue='group',style='group')"
      ],
      "metadata": {
        "id": "Rn-iivpTcwod"
      },
      "id": "Rn-iivpTcwod",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.0 Scatterplot\n",
        "sns.scatterplot(data = cw2_0, x = 'x1',y='x2',hue='group',style='group')"
      ],
      "metadata": {
        "id": "pLe-1vNYdBnV"
      },
      "id": "pLe-1vNYdBnV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Question 6 Part 3*"
      ],
      "metadata": {
        "id": "VJuLNuiIiTm9"
      },
      "id": "VJuLNuiIiTm9"
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 6 PART 3 (reference also from provided solution- giving credit where it's due!)\n",
        "def maxmin(x):\n",
        "  x = (x-min(x))/(max(x)-min(x))\n",
        "  return x\n",
        "\n",
        "def scree(data):\n",
        "  X = data.loc[ : , ['x1','x2'] ]\n",
        "  X = X.apply(maxmin)\n",
        "\n",
        "  k_bar = 15\n",
        "  k_grid = np.arange(1,k_bar+1)\n",
        "  SSE = np.zeros(k_bar)\n",
        "  for k in range(k_bar):\n",
        "    model = KMeans(n_clusters=k+1, max_iter=300, n_init = 10, random_state=0)\n",
        "    model = model.fit(X)\n",
        "    SSE[k] = model.inertia_\n",
        "  scree_plot, axes = plt.subplots()\n",
        "  sns.lineplot(x=k_grid, y=SSE).set_title('Scree Plot')\n",
        "  axes.set_ylim(0, 35)"
      ],
      "metadata": {
        "id": "NyNvi3IogApg"
      },
      "id": "NyNvi3IogApg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.125 Scree Plot\n",
        "scree( data = cw0_125 )"
      ],
      "metadata": {
        "id": "YFKnvOgchFmt"
      },
      "id": "YFKnvOgchFmt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.25 Scree Plot\n",
        "scree( data = cw0_25)"
      ],
      "metadata": {
        "id": "WWnA5iSDhQSs"
      },
      "id": "WWnA5iSDhQSs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.5 Scree Plot\n",
        "scree( data = cw0_5)\n",
        "# For these first three plots, there is a very clear elbow presenting at k=3"
      ],
      "metadata": {
        "id": "PwM338bIihxR"
      },
      "id": "PwM338bIihxR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.0 Scree Plot\n",
        "scree( data = cw1_0)"
      ],
      "metadata": {
        "id": "RGoawNztirhS"
      },
      "id": "RGoawNztirhS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.0 Scree Plot\n",
        "scree( data = cw2_0)\n",
        "# For these final two plots, there isn't really a clear elbow present? So, as the noise increases, there becomes a lack of clarity."
      ],
      "metadata": {
        "id": "GyeNXXrLiy5V"
      },
      "id": "GyeNXXrLiy5V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drawing from the noise plots done in part 2, it became clear that as the noise increases by increments, the visual distinctness of the clusters begin to get more dispersed and overlap. Correlating this information with the fact that \"elbows\" were only present in the first three plots where there was less noise, a direct relationship occurs between noise and the plausbility of scree plots accurately displaying: as noise increases, it becomes more difficult to discern k-levels."
      ],
      "metadata": {
        "id": "Ch0tTp77j088"
      },
      "id": "Ch0tTp77j088"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}